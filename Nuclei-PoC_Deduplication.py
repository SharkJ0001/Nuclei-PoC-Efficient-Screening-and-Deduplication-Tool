import os
import shutil
import hashlib
import re
import argparse
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# --- 1. ä¸“ä¸šæ—¥å¿—é…ç½® ---
# åˆ›å»ºä¸€ä¸ª logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ç»ˆç«¯å¤„ç†å™¨ï¼Œåªæ˜¾ç¤º INFO ç­‰çº§åŠä»¥ä¸Šçš„æ¶ˆæ¯
c_handler = logging.StreamHandler()
c_handler.setLevel(logging.INFO)
c_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
c_handler.setFormatter(c_formatter)

# æ–‡ä»¶å¤„ç†å™¨ï¼Œåªè®°å½• ERROR ç­‰çº§çš„æ¶ˆæ¯
f_handler = logging.FileHandler('processing_errors.log', mode='w', encoding='utf-8')
f_handler.setLevel(logging.ERROR)
f_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
f_handler.setFormatter(f_formatter)

logger.addHandler(c_handler)
logger.addHandler(f_handler)
# --- æ—¥å¿—é…ç½®ç»“æŸ ---

def process_file(file_path, collect_errors_flag, error_dir):
    """
    å¯¹å•ä¸ªæ–‡ä»¶æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥ã€‚
    æ–°å¢äº†å‚æ•°æ¥å†³å®šæ˜¯å¦æ”¶é›†é”™è¯¯æ–‡ä»¶ã€‚
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        result = {
            'path': file_path,
            'filename': os.path.basename(file_path),
            'hash': None,
            'severity': 'unknown',
            'skip_keyword': False
        }

        match_req = re.search(r'^(requests|http):', content, re.MULTILINE)
        if match_req:
            request_block_raw = content[match_req.start():]
            cleaned_lines = [line for line in request_block_raw.splitlines() if not line.strip().startswith('#')]
            condensed_content = re.sub(r'\s+', '', "\n".join(cleaned_lines))
            result['hash'] = hashlib.md5(condensed_content.encode('utf-8')).hexdigest()
        else:
            return None

        match_sev = re.search(r'^\s*severity:\s*(\w+)', content, re.MULTILINE)
        if match_sev:
            result['severity'] = match_sev.group(1).lower()

        primary_keywords = ['HTTP', 'GET', 'POST', 'PUT', 'BaseURL']
        secondary_keywords = ['/readme.txt', '/style.css']
        for line in content.splitlines():
            if any(p_kw in line for p_kw in primary_keywords) and any(s_kw in line for s_kw in secondary_keywords):
                result['skip_keyword'] = True
                break
        
        return result
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶ '{file_path}' å¤„ç†å¤±è´¥ï¼ŒåŸå› : {e}")
        
        # --- æ–°å¢æ”¹åŠ¨ ---
        # å¦‚æœç”¨æˆ·è®¾ç½®äº† --collect-errors æ ‡å¿—ï¼Œåˆ™å¤åˆ¶å‡ºé”™çš„æ–‡ä»¶
        if collect_errors_flag:
            try:
                error_dest_path = os.path.join(error_dir, os.path.basename(file_path))
                shutil.copy2(file_path, error_dest_path)
            except Exception as copy_e:
                logger.error(f"å¤åˆ¶å‡ºé”™æ–‡ä»¶ '{file_path}' åˆ° '{error_dir}' å¤±è´¥: {copy_e}")
        # --- æ–°å¢æ”¹åŠ¨ç»“æŸ ---
                
        return None

def main(args):
    """
    ä¸»æ‰§è¡Œå‡½æ•°
    """
    os.makedirs(args.output, exist_ok=True)
    
    # --- æ–°å¢æ”¹åŠ¨ ---
    # å¦‚æœè®¾ç½®äº†æ”¶é›†é”™è¯¯çš„æ ‡å¿—ï¼Œä¹Ÿåˆ›å»ºå¯¹åº”çš„ç›®å½•
    error_dir = ""
    if args.collect_errors:
        error_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "error_files")
        os.makedirs(error_dir, exist_ok=True)
        logger.info(f"é”™è¯¯æ–‡ä»¶æ”¶é›†åŠŸèƒ½å·²å¼€å¯ï¼Œå°†ä¿å­˜åˆ° '{error_dir}' ç›®å½•ã€‚")
    # --- æ–°å¢æ”¹åŠ¨ç»“æŸ ---

    logger.info(f"æ­£åœ¨ä» '{args.source}' ç›®å½•æ”¶é›†æ–‡ä»¶...")
    all_files = []
    for root, _, files in os.walk(args.source):
        for filename in files:
            if filename.endswith((".yaml", ".yml")):
                all_files.append(os.path.join(root, filename))

    if not all_files:
        logger.warning("æœªæ‰¾åˆ°ä»»ä½• .yaml/.yml æ–‡ä»¶ã€‚")
        return

    logger.info(f"å…±æ‰¾åˆ° {len(all_files)} ä¸ª PoC æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    seen_hashes = set()
    stats = {'copied': 0, 'duplicate': 0, 'severity_skipped': 0, 'keyword_skipped': 0, 'error': 0}

    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        # --- æ–°å¢æ”¹åŠ¨ ---
        # æäº¤ä»»åŠ¡æ—¶ï¼Œä¼ å…¥æ–°çš„å‚æ•°
        future_to_file = {executor.submit(process_file, f, args.collect_errors, error_dir): f for f in all_files}
        # --- æ–°å¢æ”¹åŠ¨ç»“æŸ ---
        
        for future in tqdm(as_completed(future_to_file), total=len(all_files), desc="å¤„ç† PoC"):
            result = future.result()

            if not result:
                stats['error'] += 1
                continue

            if result['hash'] in seen_hashes:
                stats['duplicate'] += 1
                continue
            
            if result['severity'] in args.exclude_severity:
                stats['severity_skipped'] += 1
                continue

            if result['skip_keyword']:
                stats['keyword_skipped'] += 1
                continue
            
            seen_hashes.add(result['hash'])
            dest_path = os.path.join(args.output, result['filename'])
            shutil.copy2(result['path'], dest_path)
            stats['copied'] += 1

    logger.info("âœ¨âœ¨âœ¨ æ‰€æœ‰é˜¶æ®µå¤„ç†å®Œæ¯•ï¼ âœ¨âœ¨âœ¨")
    logger.info(f"æœ€ç»ˆç»“æœå·²è¾“å‡ºåˆ° '{args.output}' ç›®å½•ä¸­ã€‚")
    logger.info(f"è¯¦ç»†é”™è¯¯æŠ¥å‘Šå·²ç”Ÿæˆäº 'processing_errors.log' æ–‡ä»¶ã€‚")
    if args.collect_errors:
        logger.info(f"æ‰€æœ‰å¤„ç†å‡ºé”™çš„æ–‡ä»¶å·²é›†ä¸­å¤åˆ¶åˆ° 'error_files' ç›®å½•ã€‚")
    logger.info(f"--- ç»Ÿè®¡ ---")
    logger.info(f"âœ… æˆåŠŸå¤åˆ¶æ–‡ä»¶: {stats['copied']}")
    logger.info(f"â­ï¸  å› å†…å®¹é‡å¤è·³è¿‡: {stats['duplicate']}")
    logger.info(f"ğŸš« å› ä¸¥é‡æ€§ç­‰çº§ ('{','.join(args.exclude_severity)}') è·³è¿‡: {stats['severity_skipped']}")
    logger.info(f"ğŸ—‘ï¸  å› ç‰¹å®šå…³é”®å­—è·³è¿‡: {stats['keyword_skipped']}")
    logger.info(f"âŒ å¤„ç†å‡ºé”™æ–‡ä»¶: {stats['error']}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä¸€ä¸ªé«˜æ•ˆã€çµæ´»çš„ Nuclei PoC ä¸‰é˜¶æ®µç­›é€‰å·¥å…·ã€‚")
    parser.add_argument("-s", "--source", required=True, help="å­˜æ”¾åŸå§‹ PoC çš„æºç›®å½•ã€‚")
    parser.add_argument("-o", "--output", required=True, help="å­˜æ”¾æœ€ç»ˆç­›é€‰ç»“æœçš„è¾“å‡ºç›®å½•ã€‚")
    parser.add_argument("-es", "--exclude-severity", nargs='+', default=['info'], help="éœ€è¦æ’é™¤çš„ä¸¥é‡æ€§ç­‰çº§åˆ—è¡¨ï¼Œç”¨ç©ºæ ¼åˆ†éš”ã€‚ (é»˜è®¤: info)")
    parser.add_argument("-w", "--workers", type=int, default=4, help="ç”¨äºå¤„ç†æ–‡ä»¶çš„å¹¶å‘çº¿ç¨‹æ•°ã€‚ (é»˜è®¤: 4)")
    # --- æ–°å¢æ”¹åŠ¨ ---
    parser.add_argument("--collect-errors", action="store_true", help="å¦‚æœè®¾ç½®ï¼Œåˆ™å°†æ‰€æœ‰å¤„ç†å‡ºé”™çš„æ–‡ä»¶å¤åˆ¶åˆ° 'error_files' ç›®å½•ã€‚")
    # --- æ–°å¢æ”¹åŠ¨ç»“æŸ ---
    
    args = parser.parse_args()
    main(args)